/**
 * Ollama Local LLM Chatbot - Ch·∫°y AI tr√™n m√°y local
 * KH√îNG c·∫ßn OpenAI API, ho√†n to√†n FREE & UNLIMITED
 * 
 * Setup: https://ollama.ai
 * Models: llama2, mistral, phi, gemma...
 */

const axios = require("axios");
const { Product, Category } = require("../../models");

/**
 * L·∫•y shop context
 */
const getShopContext = async () => {
    try {
        const products = await Product.find({ isActive: true })
            .populate('category', 'name')
            .select('name price fragrances category')
            .limit(50);

        const allFragrances = new Set();
        products.forEach(p => {
            if (p.fragrances && Array.isArray(p.fragrances)) {
                p.fragrances.forEach(f => allFragrances.add(f));
            }
        });

        const categories = await Category.find().select('name');

        return {
            fragrances: Array.from(allFragrances).sort(),
            categories: categories.map(c => c.name),
            products: products.map(p => ({
                name: p.name,
                price: p.price,
                fragrances: p.fragrances || [],
                category: p.category?.name || 'Ch∆∞a ph√¢n lo·∫°i'
            }))
        };
    } catch (error) {
        console.error('‚ùå L·ªói l·∫•y shop context:', error);
        return { fragrances: [], categories: [], products: [] };
    }
};

/**
 * T·∫°o system prompt cho Ollama
 */
const createSystemPrompt = (shopContext) => {
    const { fragrances, categories, products } = shopContext;

    return `B·∫°n l√† AI t∆∞ v·∫•n b√°n h√†ng th√¥ng minh c·ªßa Aura Candle - shop n·∫øn th∆°m cao c·∫•p.

TH√îNG TIN SHOP:
- T·ªïng s·∫£n ph·∫©m: ${products.length} lo·∫°i n·∫øn th∆°m
- Danh m·ª•c: ${categories.join(', ')}
- M√πi h∆∞∆°ng c√≥ s·∫µn: ${fragrances.join(', ')}

NHI·ªÜM V·ª§:
1. T∆∞ v·∫•n s·∫£n ph·∫©m ph√π h·ª£p v·ªõi nhu c·∫ßu kh√°ch
2. Gi·ªõi thi·ªáu m√πi h∆∞∆°ng d·ª±a tr√™n s·ªü th√≠ch
3. G·ª£i √Ω s·∫£n ph·∫©m ph√π h·ª£p v·ªõi t·ª´ng d·ªãp
4. Tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ n·∫øn th∆°m, c√°ch s·ª≠ d·ª•ng, b·∫£o qu·∫£n

C√ÅCH TR·∫¢ L·ªúI:
- Th√¢n thi·ªán, nhi·ªát t√¨nh, c√≥ emoji ph√π h·ª£p üïØÔ∏è
- Ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu (2-4 c√¢u)
- N·∫øu h·ªèi v·ªÅ m√πi ‚Üí List m√πi c√≥ s·∫µn
- N·∫øu h·ªèi v·ªÅ gi√° ‚Üí Recommend trong t·∫ßm gi√°
- KH√îNG b·ªãa m√πi h∆∞∆°ng kh√¥ng c√≥ trong danh s√°ch`;
};

/**
 * Handle chat v·ªõi Ollama local LLM
 */
const handleOllamaChat = async (req, res) => {
    const { message, conversationHistory = [] } = req.body;

    if (!message || message.trim() === "") {
        return res.status(400).json({ error: "Tin nh·∫Øn kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng." });
    }

    try {
        // L·∫•y shop context
        const shopContext = await getShopContext();
        const systemPrompt = createSystemPrompt(shopContext);

        // Build prompt cho Ollama (kh√¥ng c√≥ messages array nh∆∞ OpenAI)
        let fullPrompt = `${systemPrompt}\n\n`;
        
        // Add conversation history
        conversationHistory.forEach(msg => {
            if (msg.role === 'user') {
                fullPrompt += `User: ${msg.content}\n`;
            } else {
                fullPrompt += `Assistant: ${msg.content}\n`;
            }
        });
        
        fullPrompt += `User: ${message}\nAssistant:`;

        // Call Ollama API (local)
        const ollamaUrl = process.env.OLLAMA_API_URL || 'http://localhost:11434';
        const model = process.env.OLLAMA_MODEL || 'llama2'; // ho·∫∑c 'mistral', 'phi'

        console.log(`üì° Calling Ollama (${model})...`);

        const response = await axios.post(
            `${ollamaUrl}/api/generate`,
            {
                model: model,
                prompt: fullPrompt,
                stream: false,
                options: {
                    temperature: 0.7,
                    num_predict: 200 // Gi·ªõi h·∫°n ƒë·ªô d√†i reply
                }
            },
            {
                timeout: 60000 // 60s timeout (local LLM c√≥ th·ªÉ ch·∫≠m)
            }
        );

        const botReply = response.data.response;

        console.log('‚úÖ Ollama response received');

        return res.json({
            reply: botReply,
            shopContext: {
                availableFragrances: shopContext.fragrances,
                categories: shopContext.categories
            },
            source: 'ollama',
            model: model
        });
    } catch (error) {
        console.error("‚ùå L·ªói Ollama chatbot:", error.message);
        
        // Check if Ollama is not running
        if (error.code === 'ECONNREFUSED') {
            return res.status(503).json({
                error: "Ollama ch∆∞a ch·∫°y. Vui l√≤ng start Ollama: 'ollama serve'",
                hint: "Download t·∫°i: https://ollama.ai"
            });
        }

        return res.status(500).json({
            error: "Bot ƒëang b·∫≠n, th·ª≠ l·∫°i sau nh√© üòÖ",
            details: error.message
        });
    }
};

module.exports = {
    handleOllamaChat
};

